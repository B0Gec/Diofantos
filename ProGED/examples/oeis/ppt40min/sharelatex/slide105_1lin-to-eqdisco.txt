

	Uvod/ eq disco / lin-> eq disco.


  "Ce sedaj pogledam kaj ima to veze z algoritmom za odkrivanje ena"cb:

  Algoritem za odkrivanje ena"cb ima podobne korake:
- 0.) Generira seznam ena"cb. (Lin. reg. je na tem koraku "generirala"
samo eno (linearno) ena"cbo. Ostali koraki pa so zelo podobni.
- 1.) Definira nakako modela oz. ena"cbe:
Tj. "ce v splo"sni ena"cbi, ki je lahko odvisna od konstant, 
konstante fiksiramo, dobimo prirejeno napako, npr.:
	sum -\\- (y_i - izraz(c0,..,cn,x_i1,...x_in)**2
kjer so y, c, x kot pri linearni regresiji, in kjer izraz
predstavlja izraz, odvisen od c, x, ki ga je algoritem generiral v 
prej"snjem koraku.
- 2.) Tretji korak je enak kot pri Lin. reg., tj. optimizacija
konstant, da bo napaka modela "cim manj"sa. Tu se pojavi manj"sa
razlika, ker ni tako preprosto poiskati minimuma z odvodi kot pri lin. 
regresiji (oz. se mogo"ce ta opti. metoda ne izka"ze kot najbolj"sa) 
 (obstajajo bolj"se, bolj robustne).
Vendar ravno tako uporabimo nek optimizacijski algoritem, da za 
splo"sno ena"cbo (odvisno od konstant) dobimo
najbolj"so konkretno ena"cbo (s fiksnimi konstantami), v kateri 
nastopajo stolpci v vlogi spremenljivk.
- To je to. Vidimo lahko, da je algoritem zelo podnoben lin. regresiji.
le, da se tu ne omejimo samo na linearne ena"cbe.
 


** V naslednjih prosojnicah bomo videli kak"sne ena"cbe in kako pravzaprav generiramo.

** Note: Pomembna verzija tega algoritma je odkrivanje diferencialnih ena"cb.
Takrat namesto omenjene napake z vsoto najprej pri fiksnih konstantah
re"simo diferenciajno ena"cbo oblike \dot{y} = izraz(y, x1,..,xn,) 
in nato blabla ter tako dobimo nakako modela.


--- timing : ---
Sun 02 May 2021 08:57:41 AM CEST
Sun 02 May 2021 09:07:57 AM CEST = prvi slide
Sun 02 May 2021 09:14:17 AM CEST = end prva dva slajda.
--- tj. 17 min ---






