9.41

      Metoda najmanj"sih kvadratov


  Ker "ze imamo pri roki linearno regresijo, se lahko spomnimo, "ce se prav
spomnim, da je metoda najmanj"sih v resnici merilo po katerem dolo"camo, za 
koliko nek konkreten model oz. matrika odstopa od danih podatkov.
  Natan"cneje, za konkretne konstante c0, c1, c2, ... cn, ki s tem dolo"cajo 
model, nam vrednost
sum_i=1 ^m (y _i - c0 + c1*x_i1  + c2*x_i2+ ... cn*x_in )**2 
dolo"ca odstopanje od podatkov oz. napako modela.
Ta vsota je le nepregleden na"cin zapisa, da najprej za vsako vrstico podatkovne 
mno"zice izra"cunamo razliko y - c0 + c1*x1 + ... + cn*xn, kjer so
y, x1, ... xn vrednosti atributov primera v ustreznih stolpcih in nato 
to razliko kvadriramo in po se"stejemo po vrsticah.
  Pri linearni regresiji s momo"cjo te definicije napake, optimiziramo konstante
tako, da minimiziramo to napako. Za to poskrbi privzeti optimizacijski algoritem.

  Pri na"sem algoritmu za odk. en. je zgodba podobna.
Za dano ena"cbo y = izraz(x1,...,xn), ki lahko vsebuje tudi poljubno "stevilo
konstant, definiramo za konkrento dolo"cene konstante napako podobno kot pri linearni
regresiji. Tj. po vrsticah najprej kvadriramo razliko y - izraz(x1,..,xn, c1,...,ck) in nato
se"stejemo, da dobimo vsoto 
sum_i=1 ^m (y _i - izraz(x_i1,...,x_in, c0, c1, ... c_k)**2 .
Nato z privzetim optimizacijskim algoritmom minimiziramo to napako po vzeh mo"znih 
vrednosti konstant. Tako dobimo konkretno ena"cbo, in njej prirejeno napako, ki nam
jo je vrnil optimizacijski algoritem.

Opomba: To je napaka, defifirana po metodi najmanj"sih kvadratov. Lahko bi izbrali tudi
kak"sno drugo (poljubno) mero napake.

To je vse kar se ti"ce druge komponente (evalvacija ena"cb).

Kot zgled, kaj lahko po"cnemo v tretji fazi, je npr to, da iz seznama ena"cb in njihovih 
napak izberemo tiste ena"cbe, ki imajo napako velikosti pod dolo"cenim pragom in 
izberemo samo najbolj"si odstotek ena"cb. Ena"cbe znotraj tega razglasimo za
nanovo odkrite ena"be.

Opomba: Do sedaj sem "ze velikokrat sem omenjal optimizacijske algoritme, vendar nikoli
povedal podrobneje, kaj zares po"cnejo. V to se v svoji magisterski in dana"snji
predstavitvi nebom spu"s"cal, saj to ni tako blizu glavnega cilja magisterske.
((Prav tako se mi zdi, da to ni tako blizu osrednje pozornosti podro"cja strojnega u"cenja.))
Lahko pa mimogrede omenim, da sem osebno imel v tem konkretnem primeru najve"ckrat 
opravka z opnimizacijo imenovano diferen"cna evolucija (diferrential evolution), ki se
zgleda dokaj robustno obna"sa v splo"snem problemu.

-11.38




